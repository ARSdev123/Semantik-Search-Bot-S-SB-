
class Chatbot:
    """
    Classe principale du Chatbot.
    G√®re la logique de conversation, la gestion des connaissances et l'apprentissage.
    """
    def __init__(self):
        """
        Initialise le chatbot.
        - Charge les donn√©es de base (intentions, base de connaissances).
        - Charge les connaissances acquises par apprentissage.
        - Initialise les variables d'√©tat.
        """
        self.nom_user = ""
        # Charge les donn√©es depuis les fichiers JSON
        self.intents, self.knowledge_base = self._load_data('data.json', is_base=True)
        self.apprentissage = self._load_data("apprentissage.json", is_base=False)
        # Variable pour garder en m√©moire la question en attente d'une r√©ponse pour l'apprentissage
        self.question_en_attente = None
        #pour receuillir le taux de popularit√© d'une question
        self.statistiques_questions=self._load_stats()
        

    def _load_stats(self):
          if not os.path.exists("stats.json"):
            with open("stats.json", "w", encoding="utf-8") as f:
               json.dump({}, f)
          with open("stats.json", "r", encoding="utf-8") as f:
            return json.load(f)

    def _save_stats(self):
          with open("stats.json", "w", encoding="utf-8") as f:
           json.dump(self.statistiques_questions, f, ensure_ascii=False, indent=4)

    def _increment_stat(self,question):
        if question in self.statistiques_questions:
            self.statistiques_questions[question] +=1
        else:
            self.statistiques_questions[question] =1
        self._save_stats()

    def score_similarity(self, phrase, question):
        """
        Calcule le score de similarit√© entre une phrase et une question.
        Utilise SequenceMatcher pour √©valuer la similarit√©.
        """
        return SequenceMatcher(None, phrase, question).ratio()
    
    #decoupage du texte en passages
    def decouper_chunks(self, text, taille_fenetre):
        phrases=re.split(r'(?<=[.!?])\s+', text)
        chunks=[]
        for i in range (len(phrases) - taille_fenetre +1):
            chunks=''.join(phrases[i:i+taille_fenetre])
            chunks.append(chunks)
        return chunks

        
    #on vectorise toutes les phrases avec TfidVectorizer
    def score_tfidf(self, passage, question):
        vector= TfidfVectorizer(stop_words='french')
        X= vector.fit_transform(passage + [question])
        scores=(X[:-1]*X[-1].T).toarray().ravel()
        return scores

    def _load_data(self, fichier, is_base=False):
        """
        Charge les donn√©es depuis un fichier JSON.
        Cr√©e le fichier s'il n'existe pas.
        """
        # Cr√©e le fichier s'il n'existe pas (utile pour le premier lancement)
        if not os.path.exists(fichier):
            with open(fichier, "w", encoding="utf-8") as f:
                # Initialise avec un objet vide ou une structure de base
                json.dump({} if not is_base else {"intents": [], "knowledge_base": {}}, f)

        # Charge les donn√©es du fichier
        with open(fichier, "r", encoding="utf-8") as f:
            data = json.load(f)

        if is_base:
            # Pour le fichier de base, on nettoie les cl√©s de la base de connaissances
            knowledge_base = {
                self.nettoyer_message(k): v for k, v in data["knowledge_base"].items()
            }
            return data["intents"], knowledge_base
        else:
            # Pour le fichier d'apprentissage, on nettoie toutes les cl√©s
            return {self.nettoyer_message(k): v for k, v in data.items()}

    def nettoyer_message(self, text):
        """
        Nettoie une cha√Æne de caract√®res pour la rendre comparable :
        - Met en minuscules.
        - Supprime les espaces au d√©but et √† la fin.
        - Supprime les accents.
        """
        text = text.lower().strip()
        text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')
        return text

    def _sauvegarde_apprentissage(self):
        """Sauvegarde le dictionnaire d'apprentissage dans le fichier JSON."""
        with open("apprentissage.json", "w", encoding="utf-8") as f:
            json.dump(self.apprentissage, f, ensure_ascii=False, indent=4)

            
    #---FONCTION PERMETTANT DE LIRE UN FICHIER TXT---
    def lire_fichier_txt(chemin):
        try:
           with open (chemin, "r", encoding="utf-8") as f:
             contenu = f.read()
             return contenu
        except FileNotFoundError:
            return f"Le fichier {chemin} n'existe pas."
        except Exception as e:
            return f"Une erreur est survenue lors de la lecture du fichier {chemin}: {e}"
        
        
    #---FONCTION PERMETTANT DE LIRE UN DOCUMENT PDF---
    def lire_fichier_pdf(chemin):
        try:
            doc=fitz.open(chemin)
            contenu = ""
            for page in doc:
                contenu += page.get_text()
            doc.close()
            return contenu
        except FileNotFoundError:
            return f"Le fichier {chemin} n'existe pas."
        except Exception as e:
            return f"Une erreur est survenue lors de la lecture du fichier {chemin}: {e}"   
        
    #fonction de nettoyage de texte
    def nettoyer_texte(self, texte):
        """
        Nettoie le texte en supprimant les caract√®res sp√©ciaux et les espaces superflus.
        """
        # Supprimer les caract√®res sp√©ciaux
        texte = re.sub(r'[^\w\s]', '', texte)
        # Supprimer les espaces multiples
        texte = re.sub(r'\s+', ' ', texte)
        # Supprimer les espaces au d√©but et √† la fin
        return texte.strip()
    

    def extraire_passages(self, texte, question):
        """
        Extrait les passages les plus pertinents en fonction de la question.
        Am√©liorations :
        - Nettoyage avanc√© (ponctuation, accents).
        - D√©coupage en chunks (fen√™tres glissantes de 3 phrases).
        - Score de pertinence pond√©r√© (mots-cl√©s fr√©quents + proximit√©).
        - Retour limit√© aux meilleurs passages.
        """
        # 1) Pr√©parer la question
        question_nettoye = self.nettoyer_texte(question)
        question_mots = set(question_nettoye.lower().split())

        # 2) D√©couper le texte en chunks de 3 phrases
        taille_fenetre = 3
        chunks = self.decouper_chunks(texte, taille_fenetre)
        passage_score = []

        for chunk in chunks:
            chunk_nettoye = self.nettoyer_texte(chunk)
            chunk_mots = set(chunk_nettoye.lower().split())
            # 3) Calcul du score
            mots_communs = question_mots.intersection(chunk_mots)
            score_base = len(mots_communs)
            # Bonus si tous les mots-cl√©s sont pr√©sents
            score_bonus = score_base + (2 if question_mots.issubset(chunk_mots) else 0)
            if score_base > 0:
                passage_score.append((chunk.strip(), score_bonus))

        # 4) Trier par pertinence d√©croissante
        passage_score.sort(key=lambda x: x[1], reverse=True)

        # 5) Retourner les 5 passages les plus pertinents
        if not passage_score:
            return "Aucun passage retrouv√©."
        meilleurs_passages = [p[0] for p in passage_score[:5]]
        return " ".join(meilleurs_passages)


    def repondre(self, message):
        """
        G√©n√®re une r√©ponse √† un message de l'utilisateur.
        C'est le c≈ìur de la logique du chatbot.
        """
        message_nettoye = self.nettoyer_message(message)
        #mise a jour des stats de chaques question des bases de connaissances
        if message_nettoye in self.apprentissage or message_nettoye in self.knowledge_base:
            self.statistiques_questions[message_nettoye]= self.statistiques_questions.get(message_nettoye,0)+1
            self._save_stats()


        # --- √âTAPE 1 : GESTION DE L'APPRENTISSAGE ---
        # Si le bot attend une r√©ponse pour une question pr√©c√©dente...
        if self.question_en_attente:
            question = self.question_en_attente
            # On associe la question pr√©c√©dente avec la r√©ponse actuelle (le message de l'utilisateur)
            self.apprentissage[question] = {
                "reponse": message,
                "date": datetime.now().strftime("%d/%m/%Y, %H:%M:%S"),
                "theme": "inconnu"
            }
            self._sauvegarde_apprentissage()
            # On r√©initialise la question en attente, le cycle d'apprentissage est termin√© pour cette question.
            self.question_en_attente = None
            # Note : Le bot ne dit rien et continue pour r√©pondre √† la question actuelle de l'utilisateur si besoin.


        # --- √âTAPE 2 : APPRENTISSAGE DU PR√âNOM ---
        if "je m'appelle" in message_nettoye or "mon nom est" in message_nettoye :
            mots = message.split()
            for i, mot in enumerate(mots):
                if mot in ["m'appelle", "m'appelles"] and i + 1 < len(mots):
                    self.nom_user = mots[i + 1].capitalize()
                    return f"Enchant√©, {self.nom_user}. Que puis-je faire pour toi? üòÅ"
            return "Je n'ai pas compris ton pr√©nom."
        
        # Si le pr√©nom n'est pas encore connu, on le demande.
        if not self.nom_user:
            return "Salut! Je ne connais toujours pas ton pr√©nom, comment tu t'appelles?"
        
        # --- √âTAPE 2.5 : AFFICHAGE DES QUESTIONS POPULAIRES ---
        if any(mot in message_nettoye for mot in self.intents.get('faq',[])):
            exemples=list(set(self.knowledge_base.keys()) | set( self.apprentissage.keys()))

            if not exemples:
                return "je n'ai pas encore appris de questions. Pose moi la tienne"
            #Tri des questions par popularit√© decroissante:
            exemples_tries = sorted(
                 exemples,
                 key=lambda q: self.statistiques_questions.get(q, 0),
                 reverse=True
            )
            # On garde les 5 plus populaires
            top_questions = exemples_tries[:5]

             # Construction de la r√©ponse
            lignes = ["ü§ñ Voici les questions les plus souvent pos√©es :\n"]
            for q in top_questions:
                lignes.append(f"üî∏ {q.capitalize()} ({self.statistiques_questions.get(q, 0)} fois)")

            return "\n".join(lignes)

            

        # --- √âTAPE 3 : RECHERCHE DANS LES INTENTIONS DE BASE ---
        if any(mot in message_nettoye for mot in self.intents['saluer']):
            return random.choice([f"Bonjour √† toi {self.nom_user} ! üòé", f"Salut {self.nom_user}, comment tu vas?", f"Heureux de te revoir {self.nom_user}, de quoi veux-tu qu'on parle aujourd'hui?"])
        
        elif any(mot in message_nettoye for mot in self.intents['comment_cv']):
            return random.choice([f"Je vais bien merci {self.nom_user}. Et toi?", f"Aucun bug du syst√®me pour l'instant {self.nom_user} üòé", f"Je suis toujours en forme pour discuter üòÑ"])
        
        elif any(mot in message_nettoye for mot in self.intents['heure']):
            return random.choice([f"Il est {datetime.now().strftime('%d/%m/%Y, %H:%M:%S')} üëå {self.nom_user}", f"Nous sommes le {datetime.now().strftime('%d/%m/%Y, %H:%M:%S')} üëå {self.nom_user}"])
        
        elif any(mot in message_nettoye for mot in self.intents['remercier']):
            return random.choice([f"Avec plaisir {self.nom_user} üòä", f"Il n'y a pas de quoi {self.nom_user} üëå"])
        
        elif any(mot in message_nettoye for mot in self.intents['aurevoir']):
            return random.choice([f"√Ä bient√¥t {self.nom_user} ü´° !", f"J'esp√®re te revoir tr√®s vite {self.nom_user}"])
        
        elif "qu'est ce que tu sais faire?" in message_nettoye or "tu sais faire quoi?" in message_nettoye:
            return f"Et bien {self.nom_user}...\nJe suis un chatbot simple capable de te donner l'heure et de simuler une conversation simple.\nCependant je suis encore en cours de d√©veloppement, donc tr√®s limit√© üòä"
        
        elif "je vais bien merci" in message_nettoye or "√ßa va bien" in message_nettoye or "ca va" in message_nettoye:
            return random.choice([f"Vraiment heureux de l'entendre {self.nom_user}", f"Cool! Alors de quoi veux-tu qu'on parle {self.nom_user} ?"])

        # --- √âTAPE 4 : RECHERCHE EXACTE DANS LES BASES DE CONNAISSANCES ---
        if message_nettoye in self.knowledge_base:
            self._increment_stat(message_nettoye)
            return self.knowledge_base[message_nettoye]

        if message_nettoye in self.apprentissage:
            self._increment_stat(message_nettoye)
            return self.apprentissage[message_nettoye]

        # --- √âTAPE 5 : RECHERCHE PAR CORRESPONDANCE APPROXIMATIVE ---
        all_known_questions = list(self.knowledge_base.keys()) + list(self.apprentissage.keys())
        correspondance = difflib.get_close_matches(message_nettoye, all_known_questions, n=1, cutoff=0.75)
        if correspondance:
            question_proche = correspondance[0]
            self._increment_stat(question_proche)
            # On cherche la r√©ponse dans les deux dictionnaires
            return self.knowledge_base.get(question_proche) or self.apprentissage[question_proche]["reponse"]

        # --- √âTAPE 6 : QUESTION INCONNUE, ON D√âCLENCHE L'APPRENTISSAGE ---
        self.question_en_attente = message_nettoye
        return "Je ne connais pas encore cette question. Veux-tu m‚Äôenseigner la r√©ponse ? Envoie-la moi simplement."


 def extraire_passages(self, texte, question):
        """
        Extrait les passages les plus pertinents en fonction de la question en utilisant l'analyse vectorielle TF-IDF.
        Cette m√©thode :
        1. D√©coupe le texte en chunks (fen√™tres glissantes de 3 phrases)
        2. Utilise TF-IDF pour vectoriser les chunks et la question
        3. Calcule la similarit√© cosinus entre chaque chunk et la question
        4. Retourne les passages les plus pertinents sans doublons
        """
        # V√©rifier que le texte et la question ne sont pas vides
        if not texte or not question:
            return "Aucun passage retrouv√©."
        
        # D√©couper le texte en chunks de 3 phrases
        taille_fenetre = 15
        chunks = self.decouper_chunks(texte, taille_fenetre)
        
        # V√©rifier qu'il y a des chunks
        if not chunks:
            return "Aucun passage retrouv√©."
        
        # Initialiser le vectoriseur TF-IDF
        vectorizer = TfidfVectorizer(stop_words=None, ngram_range=(1, 2))  # Inclure les bigrammes pour plus de pr√©cision
        
        try:
            # Vectoriser tous les chunks + la question
            # La question est ajout√©e √† la fin de la liste pour pouvoir l'identifier facilement
            X = vectorizer.fit_transform(chunks + [question])
            
            # Calculer la similarit√© cosinus entre chaque chunk et la question
            # X[:-1] correspond aux chunks, X[-1] correspond √† la question
            similarities = (X[:-1] * X[-1].T).toarray().ravel()
            
            # Cr√©er une liste de tuples (chunk, similarit√©) et la trier par similarit√© d√©croissante
            chunk_similarity_pairs = list(zip(chunks, similarities))
            chunk_similarity_pairs.sort(key=lambda x: x[1], reverse=True)
            
            # ---Retourner les passages sans doublons---

            # Utiliser un ensemble pour suivre les passages d√©j√† vus
            seen_passages = set()
            unique_passages = []
            
            for passage, similarity in chunk_similarity_pairs:
                # Ne consid√©rer que les passages avec une similarit√© significative
                if similarity > 0.1 and passage not in seen_passages:
                    unique_passages.append(passage)
                    seen_passages.add(passage)
                    
                    # Limiter √† 3 passages uniques pour √©viter la redondance
                    if len(unique_passages) >= 3:
                        break
            
            # Si aucun passage pertinent n'a √©t√© trouv√©, retourner un message appropri√©
            if not unique_passages:
                return "Aucun passage pertinent trouv√© dans le document."
            
            # Joindre les passages uniques avec des sauts de ligne
            return "\n\n".join(unique_passages)
            
        except Exception as e:
            # En cas d'erreur dans la vectorisation, retourner un message d'erreur
            return f"Erreur lors de l'analyse du document: {str(e)}"


            def _extraire(self, chemin_pdf, seuil_taille=14):
    """
    Extrait les titres d'un PDF en se basant sur la taille de police.
    """
    try:
        doc = fitz.open(chemin_pdf)
        titres = []
        
        for page in doc:
            blocks = page.get_text("dict")["blocks"]
            for block in blocks:
                if "lines" in block:
                    for line in block["lines"]:
                        ligne_texte = ""
                        taille_max = 0
                        for span in line["spans"]:
                            ligne_texte += span["text"]
                            if span["size"] > taille_max:
                                taille_max = span["size"]
                        
                        # Si la taille est suffisante et que le texte n'est pas vide
                        if taille_max >= seuil_taille and ligne_texte.strip():
                            texte_net = ligne_texte.strip()
                            if texte_net not in titres:
                                titres.append(texte_net)
        doc.close()
        return titres
    except Exception as e:
        print(f"Erreur lecture PDF : {e}")
        return []


        extraire la premiere page

         def extraire_premiere_page(self, chemin_pdf):
        """
        Extrait le texte de la premi√®re page d'un document PDF.
        """
        try:
            doc = fitz.open(chemin_pdf)
            if len(doc) > 0:
                page_1 = doc[0]
                contenu = page_1.get_text()
                doc.close()
                return contenu
            else:
                doc.close()
                return "Document PDF vide."
        except Exception as e:
            print(f"Erreur lors de l'extraction de la premi√®re page : {e}")
            return ""





 #fonction permettant d'identifier une question vague 
    def est_question_vague(self, question):
        """
        D√©termine si une question est vague en fonction de la pr√©sence de mots interrogatifs.
        """
        question= question.lower().strip()
        #questions tres courtes (souvent vagues)
        if len(question.split()) <= 7:
            return True
         # Mots indicateurs de questions vagues
        mots_vagues = [
          "parle", "explique", "quoi", "quoi sur", "parles", 
          "c'est quoi", "qu'est-ce que", "comment √ßa", "raconte",
          "parle moi", "dis moi", "montre", "montre moi"
       ]
    
        # Mots indicateurs de questions pr√©cises
        mots_precis = [
         "delai", "montant", "procedure", "taux", "combien", 
         "comment faire", "etape", "obligatoire", "limite",
         "plafond", "duree", "sanction", "mainlevee"
      ]
        
        if any(mot in question for mot  in mots_precis):
            return False
        if any(mot in question for mot in mots_vagues):
            return True
        return False


         self.fichiers_word = [
            "data/testpy.docx"
        ]

        self.fichiers_pdf = [
            
        ]
        self.fichiers_txt=[
            "data/Circulaire des informations relatives √† la CDEC.txt"
        ]

        for fichier in self.fichiers_txt:
            if os.path.exists(fichier):
                print(f"chargement du fichier text {fichier}")
                self.lire_fichier_txt(fichier)
        for fichier in self.fichiers_word:
            if os.path.exists(fichier):
                print(f"Chargement du fichier Word : {fichier}")
                self.lire_fichier_word(fichier)
        for fichier in self.fichiers_pdf:
            if os.path.exists(fichier):
                print(f"Chargement du fichier PDF : {fichier}")
                self.lire_fichier_pdf(fichier)
